{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Đọc dữ liệu\n",
    "def read_data(directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        sentiment_dir = os.path.join(directory, sentiment)\n",
    "        for filename in os.listdir(sentiment_dir):\n",
    "            with open(os.path.join(sentiment_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1 if sentiment == 'pos' else 0)\n",
    "    return texts, labels\n",
    "\n",
    "# 2. Tạo Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. Huấn luyện mô hình\n",
    "def train_model(model, train_dataloader, val_dataloader, device, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_dataloader)}\")\n",
    "        \n",
    "        # Đánh giá trên tập validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# 4. Chính\n",
    "def main():\n",
    "    # Đường dẫn đến thư mục dữ liệu\n",
    "    train_dir = 'path/to/train'\n",
    "    test_dir = 'path/to/test'\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    train_texts, train_labels = read_data(train_dir)\n",
    "    test_texts, test_labels = read_data(test_dir)\n",
    "    \n",
    "    # Chuẩn bị mô hình và tokenizer\n",
    "    model_name = 'vinai/phobert-base'  # hoặc bất kỳ mô hình BERT tiếng Việt nào khác\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # Chuẩn bị dataset và dataloader\n",
    "    max_length = 128\n",
    "    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Huấn luyện mô hình\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    train_model(model, train_dataloader, test_dataloader, device, epochs=3)\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    model.save_pretrained('path/to/save/model')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
